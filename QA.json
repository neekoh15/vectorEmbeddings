{
    "questions": [
        "What is vectorization by embeddings?",
        "How are two embeddings compared?",
        "What are word embeddings?",
        "What is a sentence embedding?",
        "What are the common applications of embeddings?",
        "How are embeddings trained?",
        "What is the Word2Vec model?",
        "How does GloVe differ from Word2Vec?",
        "What is contextual embedding?",
        "Why are embeddings useful in natural language processing?"
    ],
    "answers": [
        "Vectorization by embeddings refers to representing words, phrases, or documents as vectors in a high-dimensional space. These vectors capture the semantics and contextual relationships between words.",
        "Embeddings can be compared using similarity measures like cosine similarity. The closer the vectors are in space, the more similar they are in terms of meaning.",
        "Word embeddings are vector representations of words in a continuous space where semantically similar words are mapped close to each other.",
        "A sentence embedding is a vector representation of a complete sentence. It captures the overall meaning of the sentence, unlike word embeddings that represent individual words.",
        "Embeddings are used in a variety of applications, such as semantic search, text classification, machine translation, and sentiment analysis, among others.",
        "Embeddings are typically trained using large text datasets and algorithms like Word2Vec, GloVe, or more recent models like BERT and ELMo.",
        "Word2Vec is a popular algorithm for learning word embeddings from large text datasets. It uses surrounding word contexts to learn vector representations.",
        "GloVe, which stands for 'Global Vectors,' is a method for learning embeddings that focuses on global statistics of the corpus. Unlike Word2Vec, which relies on local contexts, GloVe constructs co-occurrence matrices and factorizes them to obtain embeddings.",
        "Contextual embedding is a vector representation that takes into account the context in which a word appears. Unlike traditional embeddings that have a fixed vector for each word, contextual embeddings can vary depending on the context.",
        "Embeddings are useful in natural language processing because they provide a dense and continuous representation of text that captures meaning and semantic relationships, making it easier to perform complex tasks."
    ]
}
